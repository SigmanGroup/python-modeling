{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T03:53:31.940766Z",
     "start_time": "2019-07-07T03:53:31.935868Z"
    }
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-17T21:29:45.261446Z",
     "start_time": "2021-11-17T21:29:45.239505Z"
    }
   },
   "outputs": [],
   "source": [
    "# stdlib\n",
    "import re\n",
    "import copy\n",
    "import random\n",
    "import itertools\n",
    "import statistics\n",
    "import multiprocessing\n",
    "\n",
    "# data wrangling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# custom\n",
    "from hotspot_classes import Hotspot\n",
    "import hotspot_utils\n",
    "import mlr_utils\n",
    "\n",
    "# Set the number of processors to use for parallel processing\n",
    "n_processors = max([1,multiprocessing.cpu_count()-2])\n",
    "\n",
    "# Read in the insults list\n",
    "with open('insults.txt', 'r') as f:\n",
    "    insults = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data - Works for single or double file layouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell assumes that your spreadsheets are in .xlsx format and that there are no columns after the parameters\n",
    "# or rows after the last reaction. Extra rows and columns before the data are fine and can be skipped with the\n",
    "# vs_parameters_header_rows and vs_parameters_start_col variables.\n",
    "# Check cell outputs to make sure everything looks good\n",
    "\n",
    "parameters_file = \"Multi-Threshold Analysis Data\" # Excel file to pull parameters from\n",
    "parameters_sheet = \"Suzuki Yields and Parameters\" # Sheet in the Excel file to pull parameters from\n",
    "parameters_start_col = 2   # 0-indexed column number where the parameters start\n",
    "parameters_y_label_col = 0  # 0-indexed column number where the ligand labels are\n",
    "parameters_header_rows = 0 # Number of rows to skip when reading the parameters\n",
    "\n",
    "response_file = \"Multi-Threshold Analysis Data\" # Excel file to pull responses from\n",
    "response_sheet = \"Suzuki Yields and Parameters\" # Sheet in the Excel file to pull responses from\n",
    "response_col = 1 # 0-indexed column number for the responses\n",
    "response_y_label_col = 0  # 0-indexed column number where the ligand labels are\n",
    "response_header_rows = 0 # Number of rows to skip when reading the responses\n",
    "\n",
    "RESPONSE_LABEL = \"Yield (%)\" # Name of your response variable\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "# EDIT ABOVE THIS LINE\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Actually start reading stuff into dataframes\n",
    "parameters_df = pd.read_excel(\"./InputData/\" + parameters_file + \".xlsx\",\n",
    "                              parameters_sheet,\n",
    "                              header = parameters_header_rows,\n",
    "                              index_col = parameters_y_label_col,\n",
    "                              )\n",
    "response_df = pd.read_excel(\"./InputData/\" + response_file + \".xlsx\",\n",
    "                            response_sheet,\n",
    "                            header = response_header_rows,\n",
    "                            index_col = response_y_label_col,\n",
    "                            usecols = list(range(0, response_col + 1))\n",
    "                            )\n",
    "\n",
    "\n",
    "# Drop any columns before parameters_start_col that are not the index column\n",
    "parameters_columns_to_keep = [col for col in range(0, len(parameters_df.columns)) if col >= parameters_start_col-1]\n",
    "parameters_df = parameters_df.iloc[:,parameters_columns_to_keep]\n",
    "\n",
    "# Combine the two dataframes into the master dataframe\n",
    "response_df.drop(response_df.columns[0:response_col-1], axis = 'columns', inplace = True)\n",
    "data_df = response_df.merge(parameters_df, left_index = True, right_index = True)\n",
    "data_df.rename(columns = {data_df.columns.values[0]: RESPONSE_LABEL}, inplace = True) # Converts the output column name from whatever it is on the spreadsheet\n",
    "data_df.dropna(inplace = True) # Remove any rows with blanks\n",
    "\n",
    "# This converts all the data to numeric values since it was reading them in as non-numeric objects for some reason\n",
    "for column in data_df.columns:\n",
    "    data_df[column] = pd.to_numeric(data_df[column], errors='coerce')\n",
    "\n",
    "# Get a list of all the features\n",
    "all_features = list(data_df.columns)\n",
    "all_features.remove(RESPONSE_LABEL)\n",
    "\n",
    "# Check for duplicate reaction labels or column names\n",
    "error = False\n",
    "if len(list(data_df.index)) != len(list(set(data_df.index))):\n",
    "    print('THERE ARE DUPLICATE REACTION LABELS IN THE DATA. PLEASE CORRECT THIS IN YOUR SPREADSHEET.')\n",
    "    error = True\n",
    "if len(list(data_df.columns)) != len(list(set(data_df.columns))):\n",
    "    print('THERE ARE DUPLICATE COLUMN NAMES IN THE DATA. PLEASE CORRECT THIS IN YOUR SPREADSHEET.')\n",
    "    error = True\n",
    "\n",
    "if not error:\n",
    "    # Print out the data distribution of the response variable\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.hist(data_df[RESPONSE_LABEL], color='grey')\n",
    "    plt.xlabel(RESPONSE_LABEL, fontsize=14)\n",
    "    plt.ylabel(\"Frequency\", fontsize=14)\n",
    "    plt.show()\n",
    "\n",
    "    # Display the dataframe\n",
    "    display(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histograms and univariate correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {features} should be a list of feature names or column numbers you want to get the distribution and correlation for\n",
    "features = all_features # Cycles through all features\n",
    "# features = list(range(4,16)) + [27, 32] # Cycles through a specific set of features by column number\n",
    "# features = ['vbur_vbur_min', 'vmin_vmin_boltz'] # Cycles through a specific set of features by name\n",
    "\n",
    "r2_cutoff = 0.3 # R^2 cutoff for correlation below which the feature will be skipped\n",
    "\n",
    "# This feels like a relic of a bygone era, but here it is. 'matplotlib' refers to what was once the 'OG' cell.\n",
    "visualization_type = 'seaborn' # 'matplotlib' or 'seaborn'\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "# EDIT ABOVE THIS LINE\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Convert all features to column names\n",
    "features = mlr_utils.process_features(features, data_df)\n",
    "print(features)\n",
    "\n",
    "anything = False\n",
    "for feature in features:\n",
    "    print(feature)\n",
    "\n",
    "    # If the feature has no variance it would break the math later on, so we skip it\n",
    "    if data_df[feature].std() == 0:\n",
    "        print(\"No variance in feature. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Get stats and line of best fit\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(data_df[feature], data_df[RESPONSE_LABEL])\n",
    "    fit_line = intercept+slope*data_df[feature]\n",
    "\n",
    "    # Skip the rest if R^2 is below the cutoff\n",
    "    if r_value**2 < r2_cutoff:\n",
    "        print(\"R^2 below cutoff. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    plt.figure(figsize=(9, 4))\n",
    "    \n",
    "    # Plot feature distribution\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.hist(data_df[feature], bins=15)\n",
    "    plt.ylabel(\"frequency\",fontsize=15)\n",
    "    plt.xlabel(f'{feature}',fontsize=15)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.xticks(fontsize=12)\n",
    "\n",
    "    # Plot univariate correlation\n",
    "    plt.subplot(1,2,2)\n",
    "\n",
    "    if visualization_type == 'matplotlib':\n",
    "        plt.scatter(data_df[feature], data_df[RESPONSE_LABEL],color=\"black\",marker=\"s\",alpha=0.5)    \n",
    "        plt.plot(data_df[feature],fit_line,color=\"black\")\n",
    "    elif visualization_type == 'seaborn':\n",
    "        sns.set_style(\"white\")\n",
    "        sns.regplot(x=data_df[feature],y=data_df[RESPONSE_LABEL],ci=95,truncate=False)\n",
    "        x_max=np.max(data_df[feature])\n",
    "        x_min=np.min(data_df[feature])\n",
    "        y_max=np.max(data_df[RESPONSE_LABEL])\n",
    "        y_min=np.min(data_df[RESPONSE_LABEL])\n",
    "        delta_x = 0.05 * (x_max-x_min)\n",
    "        delta_y = 0.05 * (y_max-y_min)\n",
    "        plt.xlim([x_min-delta_x,x_max+delta_x])\n",
    "        plt.ylim([y_min-delta_y,y_max+delta_y])\n",
    "    else:\n",
    "        print(f'Invalid visualization type: {visualization_type}. Please fix and try again.')\n",
    "        break\n",
    "\n",
    "    plt.xlabel(f'{feature}',fontsize=15)\n",
    "    plt.ylabel(RESPONSE_LABEL,fontsize=15)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.xticks(fontsize=12) \n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()  \n",
    "    anything = True  \n",
    "\n",
    "    if p_value > 0.01:\n",
    "        print(\"R^2 = {:.2f}; p-value = {:.2f}\".format(r_value**2,p_value))\n",
    "    else:\n",
    "        print(\"R^2 = {:.2f}; p-value = {:.2E}\".format(r_value**2,p_value))\n",
    "    print(\"\\n-------------------------------------------------------------------------------\\n\") \n",
    "\n",
    "if anything:\n",
    "    print('At least one result!') \n",
    "else:\n",
    "    print('No results!') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot a feature vs. another feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {features_1} and {features_2} are the features you want to get the correlation for\n",
    "# Set them to all_features to get the correlation for all features, or specify a list of features by name or column number to compare\n",
    "\n",
    "features_1 = all_features\n",
    "features_2 = all_features\n",
    "\n",
    "# features_1 = list(range(4,16)) + [27, 32] # Cycles through a specific set of features by column number\n",
    "# features_2 = ['vbur_vbur_min', 'vmin_vmin_boltz'] # Cycles through a specific set of features by name\n",
    "\n",
    "r2_cutoff = 0.3 # R^2 cutoff for correlation below which the feature pair will be skipped\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "# EDIT ABOVE THIS LINE\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Convert all features to column names\n",
    "features_1 = mlr_utils.process_features(features_1, data_df)\n",
    "features_2 = mlr_utils.process_features(features_2, data_df)\n",
    "\n",
    "for feature_1,feature_2 in itertools.product(features_1, features_2):\n",
    "    # No use wasting time comparing a feature to itself\n",
    "    if feature_1 == feature_2:\n",
    "        continue\n",
    "\n",
    "    print(f'{feature_1} - {feature_2}')\n",
    "\n",
    "    # If either feature has no variance it would break the math later on, so we skip it\n",
    "    if data_df[feature_1].std() == 0 or data_df[feature_2].std() == 0:\n",
    "        continue\n",
    "\n",
    "    # Get stats and line of best fit\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(data_df[feature_1], data_df[feature_2])\n",
    "    fit_line = intercept+slope*data_df[feature_1]\n",
    "\n",
    "    # Skip the rest if R^2 is below the cutoff\n",
    "    if r_value**2 < r2_cutoff:\n",
    "        print(\"R^2 below cutoff. Skipping.\")\n",
    "        print(\"\\n-------------------------------------------------------------------------------\\n\")\n",
    "        continue\n",
    "    \n",
    "    # Print p-values and R^2 values\n",
    "    if p_value > 0.01:\n",
    "        print(\"R^2 = {:.2f}; p-value = {:.2f}\".format(r_value**2,p_value))\n",
    "    else:\n",
    "        print(\"R^2 = {:.2f}; p-value = {:.2E}\".format(r_value**2,p_value))\n",
    "\n",
    "    plt.figure(figsize=(16, 4))\n",
    "\n",
    "    # Plot feature 1 distribution\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.hist(data_df[feature_1], bins=15)\n",
    "    plt.ylabel(\"frequency\",fontsize=15)\n",
    "    plt.xlabel(feature_1,fontsize=15)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.xticks(fontsize=12)\n",
    "\n",
    "    # Plot feature 2 distribution\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.hist(data_df[feature_2], bins=15)\n",
    "    plt.ylabel(\"frequency\",fontsize=15)\n",
    "    plt.xlabel(feature_2,fontsize=15)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.xticks(fontsize=12)\n",
    "\n",
    "    # Plot bivariate correlation\n",
    "    plt.subplot(1,3,3)\n",
    "    sns.set_style(\"white\")\n",
    "    sns.scatterplot(x=data_df[feature_1], y=data_df[feature_2], hue=data_df[RESPONSE_LABEL], palette=\"viridis\")\n",
    "    sns.regplot(x=data_df[feature_1],y=data_df[feature_2],scatter=False,truncate=False)\n",
    "\n",
    "    x_max=np.max(data_df[feature_1])\n",
    "    x_min=np.min(data_df[feature_1])\n",
    "    y_max=np.max(data_df[feature_2])\n",
    "    y_min=np.min(data_df[feature_2])\n",
    "    delta_x = 0.05 * (x_max-x_min)\n",
    "    delta_y = 0.05 * (y_max-y_min)\n",
    "    plt.xlim([x_min-delta_x,x_max+delta_x])\n",
    "    plt.ylim([y_min-delta_y,y_max+delta_y])\n",
    "\n",
    "    plt.xlabel(feature_1,fontsize=15)\n",
    "    plt.ylabel(feature_2,fontsize=15)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.xticks(fontsize=12) \n",
    "\n",
    "    # Show the plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"\\n-------------------------------------------------------------------------------\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run to see how correlated the features are with each other\n",
    "\n",
    "corrmap = data_df.corr()\n",
    "\n",
    "plt.subplots(figsize=(15,15))\n",
    "sns.heatmap(corrmap,center=0, annot=False, cmap=\"coolwarm\", cbar=True) #linewidths=0.5\n",
    "plt.xticks(range(len(all_features)),all_features, fontsize=5, rotation=90)\n",
    "plt.yticks(range(len(all_features)),all_features, fontsize=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threshold Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split options are 'random', 'ks', 'y_equidistant', 'stratified', 'define', 'none'\n",
    "\n",
    "# Core options\n",
    "split = \"ks\"\n",
    "test_ratio = 0.3 \n",
    "\n",
    "# Less common parameters\n",
    "stratified_quantiles = 10 # Number of quantiles to split the data into for stratified sampling\n",
    "defined_training_set = [] # If you want to manually define the training set, put the list of points here\n",
    "defined_test_set = [] # If you want to manually define the test set, put the list of points here\n",
    "randomstate = 42 # Random state for reproducibility\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "training_set, validation_set, test_set = hotspot_utils.train_test_splits(data_df, split, test_ratio, all_features, RESPONSE_LABEL,\n",
    "                                                        defined_test_set=defined_test_set, defined_training_set=defined_training_set,\n",
    "                                                        randomstate=randomstate, stratified_quantiles=stratified_quantiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Threshold Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cutoff in your output for what counts as an active ligand\n",
    "y_cut = 10\n",
    "\n",
    "# Set to True if you want points below the y-cut to be considered active\n",
    "low_is_good = False\n",
    "\n",
    "# How heavily to value active ligands (1) over inactive ligands (0)\n",
    "class_weight = {1:10, 0:1} \n",
    "\n",
    "# How the prune_hotspots and find_best_hotspots evaluates which are the best\n",
    "# Can be set to 'accuracy', 'weighted_accuracy', 'f1', and 'weighted_f1'\n",
    "evaluation_method = 'weighted_accuracy'\n",
    "\n",
    "# How many threshold dimensions do you want?\n",
    "n_thresholds = 2\n",
    "\n",
    "# What percentage of thresholds are analyzed in each subsequent step\n",
    "percentage = 100\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "# EDIT ABOVE THIS LINE\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Set up y_class, the binary list of which y values are above y_cut\n",
    "hotspot_data_df = copy.deepcopy(data_df)\n",
    "hotspot_data_df['y_class'] = 0\n",
    "\n",
    "if(low_is_good):\n",
    "    for i in hotspot_data_df.index:\n",
    "        hotspot_data_df.loc[i, 'y_class'] = int(hotspot_data_df.loc[i, RESPONSE_LABEL] < y_cut)\n",
    "else:\n",
    "    for i in hotspot_data_df.index:\n",
    "        hotspot_data_df.loc[i, 'y_class'] = int(hotspot_data_df.loc[i, RESPONSE_LABEL] > y_cut)\n",
    "\n",
    "# Find the best thresholds within the full X and y space and make single threshold hotspot objects from them\n",
    "all_thresholds = hotspot_utils.threshold_generation(hotspot_data_df, class_weight, evaluation_method, all_features)\n",
    "best_hotspots = []\n",
    "for thresh in all_thresholds:\n",
    "    temp_hs = Hotspot(hotspot_data_df, [thresh], y_cut, training_set, test_set, evaluation_method, class_weight)\n",
    "    best_hotspots.append(temp_hs)\n",
    "\n",
    "# Cut down to the best {percentage} hotspots\n",
    "best_hotspots = hotspot_utils.prune_hotspots(best_hotspots, percentage, evaluation_method)\n",
    "\n",
    "# Add more thresholds, pruning after each step for resource management\n",
    "for i in range(n_thresholds - 1):\n",
    "    with multiprocessing.Pool(processes=int(n_processors-2)) as p:\n",
    "        new_hotspots = p.starmap(hotspot_utils.hs_next_thresholds_fast, zip(best_hotspots, itertools.repeat(all_thresholds)))\n",
    "    new_hotspots = [item for sublist in new_hotspots for item in sublist] \n",
    "    \n",
    "    best_hotspots = hotspot_utils.prune_hotspots(new_hotspots, percentage, evaluation_method)\n",
    "\n",
    "best_hotspots.sort(key = lambda x: x.accuracy_dict[evaluation_method], reverse = True)\n",
    "\n",
    "# print the top 5 hotspots\n",
    "for i, hs in enumerate(best_hotspots[:5]):\n",
    "    print(f'Hotspot Index: {i}')\n",
    "    print(hs)\n",
    "    hs.print_stats()\n",
    "    print('\\n**********************************\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Threshold Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {manual_features} are the features that will be used to build the thresholds\n",
    "# It should be a list of lists, where each sublist is a set of features that can be selected for that threshold\n",
    "# You can specify features by name or column number\n",
    "\n",
    "manual_features = [['vbur_vbur_min'], ['vmin_vmin_boltz']] # This will create a double threshold with these three features\n",
    "# manual_features = [[1], list(range(85,91))] # This will make double thresholds with the first feature and each of the features 85 to 90\n",
    "\n",
    "# Cutoff for what counts as a hit\n",
    "y_cut = 10\n",
    "\n",
    "# How heavily to value hits (1) over misses (0)\n",
    "class_weight = {1:10, 0:1} \n",
    "\n",
    "# What percentage of hotspots to take through to each subsequent step\n",
    "# Only relevant if using ranges instead of specific parameters\n",
    "percentage = 100\n",
    "\n",
    "# How the prune_hotspots and find_best_hotspots evaluates which are the best\n",
    "# Can be set to 'accuracy', 'weighted_accuracy', 'f1', and 'weighted_f1'\n",
    "evaluation_method = 'weighted_accuracy'\n",
    "\n",
    "# Set to True if you want a hotspot of low output results (cold spot?)\n",
    "low_is_good = False\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "# EDIT ABOVE THIS LINE\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Set up y_class, the binary list of which y values are above y_cut\n",
    "hotspot_data_df = copy.deepcopy(data_df)\n",
    "hotspot_data_df['y_class'] = 0\n",
    "\n",
    "# Convert all features to column names\n",
    "for i, feature_set in enumerate(manual_features):\n",
    "    manual_features[i] = mlr_utils.process_features(feature_set, hotspot_data_df)\n",
    "\n",
    "if(low_is_good):\n",
    "    for i in hotspot_data_df.index:\n",
    "        hotspot_data_df.loc[i, 'y_class'] = int(hotspot_data_df.loc[i, RESPONSE_LABEL] < y_cut)\n",
    "else:\n",
    "    for i in hotspot_data_df.index:\n",
    "        hotspot_data_df.loc[i, 'y_class'] = int(hotspot_data_df.loc[i, RESPONSE_LABEL] > y_cut)\n",
    "\n",
    "# Find the best thresholds within the full X and y space and make single threshold hotspot objects from them\n",
    "all_thresholds = hotspot_utils.threshold_generation(hotspot_data_df, class_weight, evaluation_method, manual_features[0])\n",
    "best_hotspots = []\n",
    "for thresh in all_thresholds:\n",
    "    temp_hs = Hotspot(hotspot_data_df, [thresh], y_cut, training_set, test_set, evaluation_method, class_weight)\n",
    "    best_hotspots.append(temp_hs)\n",
    "\n",
    "# Cut down to the best {percentage} hotspots\n",
    "best_hotspots = hotspot_utils.prune_hotspots(best_hotspots, percentage, evaluation_method)\n",
    "\n",
    "# Add more thresholds, pruning after each step for resource management\n",
    "for i in range(len(manual_features) - 1):\n",
    "    new_hotspots = []\n",
    "    for hs in best_hotspots:\n",
    "        temp_hotspots = hotspot_utils.hs_next_thresholds(hs, hotspot_data_df, class_weight, manual_features[i+1])\n",
    "        new_hotspots.extend(temp_hotspots)\n",
    "    best_hotspots = new_hotspots\n",
    "    del (new_hotspots)\n",
    "    best_hotspots = hotspot_utils.prune_hotspots(best_hotspots, percentage, evaluation_method)\n",
    "    \n",
    "best_hotspots.sort(key = lambda x: x.accuracy_dict[evaluation_method], reverse = True)\n",
    "# print the top 5 hotspots\n",
    "for i, hs in enumerate(best_hotspots[:5]):\n",
    "    print(f'Hotspot Index: {i}')\n",
    "    print(hs)\n",
    "    hs.print_stats()\n",
    "    print('\\n**********************************\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For more direct control over plot style, changes can be made to the plotting functions in hotsput_utils.py\n",
    "# One of the above functions must be run before this cell can ran\n",
    "hotspot_index =  0\n",
    "print(best_hotspots[hotspot_index])\n",
    "\n",
    "# subset can be 'all', 'training', or 'test'\n",
    "# You can change the coloring to either 'scaled' or 'binary'\n",
    "hotspot_utils.plot_hotspot(best_hotspots[hotspot_index], \n",
    "                            subset='all', \n",
    "                            coloring='binary', \n",
    "                            output_label=RESPONSE_LABEL, \n",
    "                            gradient_color='Oranges')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation: Training/Test set split, Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter data before train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to go forward with only data from the active space of a (multi)threshold, run this cell\n",
    "# Select which hotspot you want to use based on the index from the Threshold Analysis section\n",
    "\n",
    "hotspot_index = 0\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "points_to_keep = best_hotspots[hotspot_index].get_hotspot_space()\n",
    "\n",
    "print(f'Number of points before filtering: {len(data_df.index)}')\n",
    "print(f'Number of points after filtering: {len(points_to_keep)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to use a custom cutoff in a certain parameter to limit which points you take forward, run this cell\n",
    "# Set keep_points_below_cutoff to True if you want to keep points below the cutoff, False if you want to keep points above\n",
    "\n",
    "cutoff_feature = 'vbur_vbur_min'\n",
    "cutoff_value = 57.277\n",
    "keep_points_below_cutoff = True\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "points_to_keep = data_df.index.to_list()\n",
    "\n",
    "for i in data_df.index:\n",
    "    if keep_points_below_cutoff:\n",
    "        if data_df.loc[i, cutoff_feature] > cutoff_value:\n",
    "            points_to_keep.remove(i)\n",
    "    else:\n",
    "        if data_df.loc[i, cutoff_feature] < cutoff_value:\n",
    "            points_to_keep.remove(i)\n",
    "\n",
    "print(f'Number of points before filtering: {len(data_df.index)}')\n",
    "print(f'Number of points after filtering: {len(points_to_keep)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to exclude specific points from both the training and test sets, run this cell\n",
    "# Points are excluded by their index label\n",
    "\n",
    "points_to_exclude = ['R1_186']\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "points_to_keep = [point for point in data_df.index if point not in points_to_exclude]\n",
    "\n",
    "print(f'Number of points before filtering: {len(data_df.index)}')\n",
    "print(f'Number of points after filtering: {len(points_to_keep)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Set Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split options are 'random', 'ks', 'y_equidistant', 'stratified', 'define', 'none'\n",
    "# If you want to pull the train and test sets from only the subset of your data defined above, set use_filtering to True\n",
    "\n",
    "# Core options\n",
    "split = \"ks\"\n",
    "test_ratio = 0.3 \n",
    "\n",
    "# Some flags\n",
    "use_filtering = False\n",
    "use_scaling = True\n",
    "\n",
    "# Less common parameters\n",
    "stratified_quantiles = 10 # Number of quantiles to split the data into for stratified sampling\n",
    "defined_training_set = [] # If you want to manually define the training set, put the list of points here\n",
    "defined_test_set = [] # If you want to manually define the test set, put the list of points here\n",
    "randomstate = 42 # Random state for reproducibility\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "if use_filtering:\n",
    "    training_set, validation_set, test_set = hotspot_utils.train_test_splits(data_df, split, test_ratio, all_features, RESPONSE_LABEL,\n",
    "                                                            defined_test_set=defined_test_set, defined_training_set=defined_training_set,\n",
    "                                                            randomstate=randomstate, stratified_quantiles=stratified_quantiles, subset=points_to_keep)\n",
    "else:\n",
    "    training_set, validation_set, test_set = hotspot_utils.train_test_splits(data_df, split, test_ratio, all_features, RESPONSE_LABEL,\n",
    "                                                            defined_test_set=defined_test_set, defined_training_set=defined_training_set,\n",
    "                                                            randomstate=randomstate, stratified_quantiles=stratified_quantiles)\n",
    "\n",
    "if use_scaling:\n",
    "    # Scales the parameters then puts them into modeling_data_df\n",
    "    scaler = StandardScaler() # If you want to use a different scaler, change this line\n",
    "\n",
    "    scaler.fit(data_df.loc[training_set, :])\n",
    "    modeling_data = scaler.transform(data_df)\n",
    "    modeling_data_df = pd.DataFrame(modeling_data, index = data_df.index, columns = data_df.columns)\n",
    "    modeling_data_df[RESPONSE_LABEL] = data_df[RESPONSE_LABEL]\n",
    "else:\n",
    "    modeling_data_df = data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear modelling - MLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {features_selected} should be a list of features (names or column numbers) to include in the model\n",
    "\n",
    "features_selected = [4, 64, 72]\n",
    "# features_selected = ['vbur_vbur_min', 'vmin_vmin_boltz']\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "# EDIT ABOVE THIS LINE UNLESS YOU WANT TO CHANGE THE PLOTTING DETAILS IN mlr_utils.plot_MLR_model(...) BELOW\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "features_selected = mlr_utils.process_features(features_selected, modeling_data_df)\n",
    "\n",
    "# Break up the train/test data into smaller dataframes for easy reference\n",
    "x_train = modeling_data_df.loc[training_set, features_selected] # Dataframe containing just the parameters used in this model for the ligands used in the training set\n",
    "x_test = modeling_data_df.loc[test_set, features_selected] # Dataframe containing just the parameters used in this model for the ligands used in the test set\n",
    "y_train = modeling_data_df.loc[training_set, RESPONSE_LABEL]\n",
    "y_test = modeling_data_df.loc[test_set, RESPONSE_LABEL]\n",
    "\n",
    "# Set up MLR and predict train/test\n",
    "lr = LinearRegression().fit(x_train, y_train)\n",
    "y_predictions_train = lr.predict(x_train)\n",
    "try:\n",
    "    y_predictions_test =  lr.predict(x_test)\n",
    "except ValueError as e:\n",
    "    print(f'Error: {e}')\n",
    "    print('This cell is not currently equiped to handle modeling without a test set.')\n",
    "    raise mlr_utils.StopExecution\n",
    "\n",
    "# Calculate q2 and k-fold for the model\n",
    "q2, loo_train = mlr_utils.calculate_q2(x_train, y_train)\n",
    "k_fold_scores = mlr_utils.repeated_k_fold(x_train, y_train, k=5, n=100)\n",
    "\n",
    "# Set up the results dataframe\n",
    "columns = ['Model', 'n_terms', 'R^2', 'Q^2', 'MAE', 'Test R^2']\n",
    "results = pd.DataFrame(columns = columns)\n",
    "results.loc[0] = [tuple(features_selected), len(features_selected), lr.score(x_train, y_train), q2, metrics.mean_absolute_error(y_train, y_predictions_train), mlr_utils.external_r2(y_test,y_predictions_test,y_train)]\n",
    "\n",
    "# Print a bunch of stats about the model\n",
    "print(f\"\\nSplit method: {split}\")\n",
    "print(f\"Test ratio: {test_ratio}\")\n",
    "print(f'Used scaling: {use_scaling}')\n",
    "\n",
    "print(f'\\nParameters:\\n{lr.intercept_:10.4f} +')\n",
    "for i, parameter in enumerate(features_selected):\n",
    "    print(f'{lr.coef_[i]:10.4f} * {parameter}')\n",
    "\n",
    "print(f\"\\nTraining R2  = {lr.score(x_train, y_train):.3f}\")\n",
    "print(f'Training Q2  = {q2:.3f}')\n",
    "print(f\"Training MAE = {metrics.mean_absolute_error(y_train, y_predictions_train):.3f}\")\n",
    "print(f'Training RMSE = {np.sqrt(metrics.mean_squared_error(y_train, y_predictions_train)):.3f}')\n",
    "print(f'Training K-fold R2 = {statistics.mean(k_fold_scores):.3f} (+/- {statistics.stdev(k_fold_scores) ** 2:.3f})')\n",
    "\n",
    "print(f\"\\nTest R2      = {mlr_utils.external_r2(y_test,y_predictions_test,y_train):.3f}\")\n",
    "print(f'Test MAE     = {metrics.mean_absolute_error(y_test,y_predictions_test):.3f}')\n",
    "print(f'Test RMSE    = {np.sqrt(metrics.mean_squared_error(y_test,y_predictions_test)):.3f}')\n",
    "\n",
    "# Print an insult if necessary\n",
    "train_r2 = lr.score(x_train, y_train)\n",
    "test_r2 = lr.score(x_test, y_test)\n",
    "if train_r2 - test_r2 > 0.35 or train_r2 < 0.4 or test_r2 < 0.2 or q2 < 0:\n",
    "    print(\"\\n\" + random.choice(insults))\n",
    "\n",
    "# Plot the model\n",
    "mlr_utils.plot_MLR_model(y_train, y_predictions_train, y_test, y_predictions_test, loo_train, output_label=RESPONSE_LABEL, plot_xy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bidirectional stepwise selection keeping a set of candidates at each step\n",
    "n_steps = 3 # This is the maximum number of parameters you want in your models\n",
    "n_candidates = 50 # This is a measure related to how many models are considered at each step. See mlr_utils.bidirectional_stepwise_regression for more details.\n",
    "collinearity_cutoff = 0.5 # This is collinearity (r^2) above which parameters won't be included in the same model\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "# EDIT ABOVE THIS LINE UNLESS YOU WANT TO CHANGE THE PLOTTING DETAILS IN mlr_utils.plot_MLR_model(...)\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# %notebook inline\n",
    "plt.ioff()\n",
    "\n",
    "# This is the function that actually does the MLR model search\n",
    "results,models,sortedmodels,candidates = mlr_utils.bidirectional_stepwise_regression(modeling_data_df.loc[training_set,:],RESPONSE_LABEL,\n",
    "                    n_steps=n_steps,n_candidates=n_candidates,collinearity_cutoff=collinearity_cutoff)\n",
    "\n",
    "# Set up the test R^2 and MAE for each model in the results dataframe\n",
    "for i in results.index:\n",
    "    model_terms = results.loc[i,\"Model\"]\n",
    "    model = models[model_terms].model\n",
    "\n",
    "    # Set the train MAE for each model\n",
    "    x_train = modeling_data_df.loc[training_set, model_terms]\n",
    "    y_train = modeling_data_df.loc[training_set, RESPONSE_LABEL]\n",
    "    y_predictions_train = model.predict(x_train)\n",
    "    results.loc[i, 'MAE'] = metrics.mean_absolute_error(y_train, y_predictions_train)\n",
    "\n",
    "    # Set test R^2 for each model\n",
    "    x_test = modeling_data_df.loc[test_set, model_terms]\n",
    "    y_test = modeling_data_df.loc[test_set, RESPONSE_LABEL]\n",
    "    test_r2 = model.score(x_test, y_test)\n",
    "    results.loc[i, 'Test R^2'] = test_r2\n",
    "\n",
    "# Identify the best model from the bidirectional_stepwise_regression algorithm\n",
    "selected_model_terms = results.loc[0, \"Model\"] # Store a tuple of 'xIDs' for the best model\n",
    "selected_model = models[selected_model_terms].model # Store the LinearRegression object for that model\n",
    "\n",
    "# Break up the train/test data into smaller dataframes for easy reference\n",
    "x_train = modeling_data_df.loc[training_set, selected_model_terms] # Dataframe containing just the parameters used in this model for the ligands used in the training set\n",
    "x_test = modeling_data_df.loc[test_set, selected_model_terms] # Dataframe containing just the parameters used in this model for the ligands used in the test set\n",
    "y_train = modeling_data_df.loc[training_set, RESPONSE_LABEL]\n",
    "y_test = modeling_data_df.loc[test_set, RESPONSE_LABEL]\n",
    "\n",
    "# Predict the train and test sets with the model\n",
    "y_predictions_train = selected_model.predict(x_train)\n",
    "try:\n",
    "    y_predictions_test =  selected_model.predict(x_test)\n",
    "except ValueError as e:\n",
    "    print(f'Error: {e}')\n",
    "    print('This cell is not currently equiped to handle modeling without a test set.')\n",
    "    raise mlr_utils.StopExecution\n",
    "\n",
    "# Calculate q2 and k-fold for the model\n",
    "q2, loo_train = mlr_utils.calculate_q2(x_train,y_train)\n",
    "k_fold_scores = mlr_utils.repeated_k_fold(x_train, y_train, k=5, n=100)\n",
    "\n",
    "# Print a bunch of stats about the model\n",
    "print(f\"\\nSplit method: {split}\")\n",
    "print(f\"Test ratio: {test_ratio}\")\n",
    "\n",
    "print(f'\\nParameters:\\n{selected_model.intercept_:10.4f} +')\n",
    "for i, parameter in enumerate(selected_model_terms):\n",
    "    print(f'{selected_model.coef_[i]:10.4f} * {parameter}')\n",
    "\n",
    "print(f\"\\nTraining R2  = {selected_model.score(x_train, y_train):.3f}\")\n",
    "print(f'Training Q2  = {q2:.3f}')\n",
    "print(f\"Training MAE = {metrics.mean_absolute_error(y_train,y_predictions_train):.3f}\")\n",
    "print(f'Training K-fold R2 = {statistics.mean(k_fold_scores):.3f} (+/- {statistics.stdev(k_fold_scores) ** 2:.3f})')\n",
    "\n",
    "print(f\"\\nTest R2      = {mlr_utils.external_r2(y_test,y_predictions_test,y_train):.3f}\")\n",
    "print(f'Test MAE     = {metrics.mean_absolute_error(y_test,y_predictions_test):.3f}')\n",
    "\n",
    "# Print an insult if necessary\n",
    "train_r2 = selected_model.score(x_train, y_train)\n",
    "test_r2 = selected_model.score(x_test, y_test)\n",
    "if train_r2 - test_r2 > 0.35 or train_r2 < 0.4 or test_r2 < 0.2 or q2 < 0:\n",
    "    print(\"\\n\" + random.choice(insults))\n",
    "    \n",
    "# Plot the model\n",
    "mlr_utils.plot_MLR_model(y_train, y_predictions_train, y_test, y_predictions_test, loo_train, output_label=RESPONSE_LABEL, plot_xy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter and View Multiple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many terms do you want in the largest model?\n",
    "max_terms = 3\n",
    "\n",
    "# What column to sort by? ('R^2', 'Q^2', 'Test R^2', 'Custom Sorter')\n",
    "sort_column = 'Q^2'\n",
    "\n",
    "# Only show models containing these features. Set to [] to show all models or a list of x# strings to filter.\n",
    "include_features = []\n",
    "\n",
    "# Should the sort be ascending or descending? (Set True if you're using MAE where lower is better, False otherwise)\n",
    "ascending = False\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "# EDIT ABOVE THIS LINE UNLESS YOU WANT TO CHANGE THE 'Custom Sorter' CALCULATION in line 16\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Calculate a custom sorter for each model\n",
    "for i in results.index:\n",
    "\n",
    "    # Set Custom Sorter function for each model.  This can be changed based on what's important to you.\n",
    "    custom_sorter = results.loc[i,'R^2'] * results.loc[i,'Q^2'] * results.loc[i,'Test R^2']\n",
    "\n",
    "    # Make sure two negative values don't cancel each other out to give a false positive\n",
    "    if(results.loc[i,'R^2'] <= 0 or results.loc[i,'Q^2'] <= 0 or results.loc[i,'Test R^2'] <= 0):\n",
    "        custom_sorter = -abs(custom_sorter)\n",
    "    results.loc[i,'Custom Sorter'] = custom_sorter\n",
    "\n",
    "# Sort and filter results\n",
    "filtered_results = results[results.n_terms <= max_terms] # Filter out models with too many terms\n",
    "\n",
    "# Filter out models that don't contain the specified features\n",
    "for feature in include_features: \n",
    "    filtered_results = filtered_results[filtered_results['Model'].apply(lambda x: feature in x)]\n",
    "\n",
    "filtered_results.sort_values(by=[sort_column], ascending=ascending, inplace=True)\n",
    "\n",
    "pd.set_option('display.width', 1000)\n",
    "print(filtered_results.head(20)) # Change this number if you want to see more models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display a Specific Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select model to display\n",
    "model_index = 4\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "# EDIT ABOVE THIS LINE UNLESS YOU WANT TO CHANGE THE PLOTTING DETAILS IN mlr_utils.plot_MLR_model(...)\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "selected_model_terms = results.loc[model_index, \"Model\"] # Store a tuple of 'xIDs' for the best model\n",
    "selected_model = models[selected_model_terms].model # Store the LinearRegression object for that model\n",
    "\n",
    "# Break up the train/test data into smaller dataframes for easy reference\n",
    "x_train = modeling_data_df.loc[training_set, selected_model_terms] # Dataframe containing just the parameters used in this model for the ligands used in the training set\n",
    "x_test = modeling_data_df.loc[test_set, selected_model_terms] # Dataframe containing just the parameters used in this model for the ligands used in the test set\n",
    "y_train = modeling_data_df.loc[training_set, RESPONSE_LABEL]\n",
    "y_test = modeling_data_df.loc[test_set, RESPONSE_LABEL]\n",
    "\n",
    "# Predict the train and test sets with the model\n",
    "y_predictions_train = selected_model.predict(x_train)\n",
    "y_predictions_test =  selected_model.predict(x_test)\n",
    "\n",
    "# Calculate q2 and k-fold for the model\n",
    "q2, loo_train = mlr_utils.calculate_q2(x_train,y_train)\n",
    "k_fold_scores = mlr_utils.repeated_k_fold(x_train, y_train, k=5, n=100)\n",
    "\n",
    "# Print a bunch of stats about the model\n",
    "print(f\"\\nSplit method: {split}\")\n",
    "print(f\"Test ratio: {test_ratio}\")\n",
    "print(f\"\\nFeatures: {selected_model_terms}\")\n",
    "\n",
    "print(f'\\nParameters:\\n{selected_model.intercept_:10.4f} +')\n",
    "for i, parameter in enumerate(selected_model_terms):\n",
    "    print(f'{selected_model.coef_[i]:10.4f} * {parameter}')\n",
    "\n",
    "print(f\"\\nTraining R2  = {selected_model.score(x_train, y_train):.3f}\")\n",
    "print(f'Training Q2  = {q2:.3f}')\n",
    "print(f\"Training MAE = {metrics.mean_absolute_error(y_train,y_predictions_train):.3f}\")\n",
    "print(f'Training K-fold R2 = {statistics.mean(k_fold_scores):.3f} (+/- {statistics.stdev(k_fold_scores) ** 2:.3f})')\n",
    "\n",
    "print(f\"\\nTest R2      = {mlr_utils.external_r2(y_test,y_predictions_test,y_train):.3f}\")\n",
    "print(f'Test MAE     = {metrics.mean_absolute_error(y_test,y_predictions_test):.3f}')\n",
    "\n",
    "# Print an insult if necessary\n",
    "trainr2 = selected_model.score(x_train, y_train)\n",
    "testr2 = selected_model.score(x_test, y_test)\n",
    "if trainr2 - testr2 > 0.35 or trainr2<0.4 or testr2<0.2 or q2<0:\n",
    "    print(\"\\n\"+random.choice(insults))\n",
    "    \n",
    "# Plot the model\n",
    "mlr_utils.plot_MLR_model(y_train, y_predictions_train, y_test, y_predictions_test, loo_train, output_label=RESPONSE_LABEL, plot_xy=True)\n",
    "\n",
    "# Print detailed model statistics\n",
    "stats_model = sm.OLS(y_train, sm.add_constant(x_train)).fit()\n",
    "print(stats_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Virtual Screening / Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Virtual Screening / Validation Parameter Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell must be executed before any other validation or virtual screening cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell assumes that your spreadsheet is in .xlsx format and that there are no columns after the parameters\n",
    "# or rows after the last reaction. Extra rows and columns before the data are fine and can be skipped with the\n",
    "# vs_parameters_header_rows and vs_parameters_start_col variables.\n",
    "\n",
    "vs_parameters_file = \"kraken descriptors\" # Excel file to pull parameters from\n",
    "vs_parameters_sheet = \"DFT_data\" # Sheet in the Excel file to pull parameters from\n",
    "vs_parameters_start_col = 1   # 0-indexed column number where the parameters start\n",
    "vs_parameters_y_label_col = 0  # 0-indexed column number where the ligand labels are\n",
    "vs_parameters_header_rows = 0 # Number of rows to skip when reading the parameters. First row should be parameter names.\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "# EDIT ABOVE THIS LINE\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Actually start reading stuff into a dataframe\n",
    "vs_parameters_df = pd.read_excel(\"./InputData/\" + vs_parameters_file + \".xlsx\",\n",
    "                              vs_parameters_sheet,\n",
    "                              header = vs_parameters_header_rows,\n",
    "                              index_col = vs_parameters_y_label_col,\n",
    "                              )\n",
    "\n",
    "# Skip any columns that are not parameters\n",
    "vs_parameters_df = vs_parameters_df.iloc[:,vs_parameters_start_col-1:]\n",
    "\n",
    "# Check that all parameters from the original dataset are present and put them in the same order\n",
    "try:\n",
    "  vs_parameters_df = vs_parameters_df[all_features]\n",
    "except KeyError:\n",
    "  print('There are features in the main dataset that are not present in this parameter file.')\n",
    "  print('If your model was built with scaled features, this will cause an error downstream.')\n",
    "\n",
    "  # If any parameters are missing, at least remove all columns that are not in the original dataset\n",
    "  columns_to_keep = [col for col in vs_parameters_df.columns if col in all_features]\n",
    "  vs_parameters_df = vs_parameters_df[columns_to_keep]\n",
    "\n",
    "# Display the dataframe\n",
    "display(vs_parameters_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell before the specific validation cells to set up the validation dataframe\n",
    "# Read the validation results from the excel sheet\n",
    "# The final result should be a dataframe with indicies corresponding to the vs_parameters_df from above,\n",
    "# a column of experimental outputs, and all the relevant parameters\n",
    "\n",
    "validation_file = \"Multi-Threshold Analysis Data\" # Excel file to pull validation results from\n",
    "validation_sheet = \"Reaction II Validation\" # Sheet in the Excel file to pull validation results from\n",
    "validation_response_col = 2 # 0-indexed column number for the experimental output\n",
    "validation_y_label_col = 0  # 0-indexed column number where the ligand labels are\n",
    "validation_header_rows = 0 # Number of rows to skip when reading the responses\n",
    "\n",
    "############################################################################################################\n",
    "\n",
    "vs_response_df = pd.read_excel('./InputData/' + validation_file + '.xlsx',\n",
    "                              sheet_name=validation_sheet,\n",
    "                              header=validation_header_rows,\n",
    "                              index_col=validation_y_label_col\n",
    "                              )\n",
    "\n",
    "# Drop all columns except the experimental output\n",
    "vs_response_df = vs_response_df.iloc[:, [validation_response_col-1]]\n",
    "vs_response_df.columns = [RESPONSE_LABEL]\n",
    "\n",
    "for column in vs_response_df.columns:\n",
    "    vs_response_df[column] = pd.to_numeric(vs_response_df[column], errors='coerce')\n",
    "vs_response_df.dropna(inplace = True)\n",
    "\n",
    "vs_combined_df = pd.concat([vs_response_df, vs_parameters_df], axis=1, join='inner')\n",
    "\n",
    "display(vs_combined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLR Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select model to validate\n",
    "model_index = 21\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "# EDIT ABOVE THIS LINE\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Scales the parameters if scaling was used in the train/test split cell\n",
    "if use_scaling:\n",
    "    vs_modeling_data = scaler.transform(vs_combined_df)\n",
    "    vs_modeling_data_df = pd.DataFrame(vs_modeling_data, index = vs_combined_df.index, columns = vs_combined_df.columns)\n",
    "    vs_modeling_data_df[RESPONSE_LABEL] = vs_combined_df[RESPONSE_LABEL]\n",
    "else:\n",
    "    vs_modeling_data_df = vs_combined_df\n",
    "\n",
    "# Select the model to use\n",
    "selected_model_terms = results.loc[model_index, \"Model\"] # Store a tuple of 'xIDs' for the best model\n",
    "\n",
    "# Break up the train/test data into smaller dataframes for easy reference\n",
    "x_train = modeling_data_df.loc[training_set, selected_model_terms] # Dataframe containing just the parameters used in this model for the ligands used in the training set\n",
    "x_test = modeling_data_df.loc[test_set, selected_model_terms] # Dataframe containing just the parameters used in this model for the ligands used in the test set\n",
    "y_train = modeling_data_df.loc[training_set, RESPONSE_LABEL]\n",
    "y_test = modeling_data_df.loc[test_set, RESPONSE_LABEL]\n",
    "\n",
    "x_validate = vs_modeling_data_df.loc[:, selected_model_terms] # Dataframe containing just the parameters used in this model for the ligands used in the validation set\n",
    "y_validate = vs_modeling_data_df.loc[:, RESPONSE_LABEL]\n",
    "\n",
    "# Set up MLR and predict train/test\n",
    "lr = LinearRegression().fit(x_train, y_train)\n",
    "y_predictions_train = lr.predict(x_train)\n",
    "if len(x_test) > 0: # In case there is no test set\n",
    "    y_predictions_test =  lr.predict(x_test)\n",
    "y_predictions_validate = lr.predict(x_validate)\n",
    "\n",
    "# Calculate q2 and k-fold for the model\n",
    "q2, loo_train = mlr_utils.calculate_q2(x_train, y_train)\n",
    "k_fold_scores = mlr_utils.repeated_k_fold(x_train, y_train, k=5, n=100)\n",
    "\n",
    "# Print a bunch of stats about the model\n",
    "print(f\"\\nSplit method: {split}\")\n",
    "print(f\"Test ratio: {test_ratio}\")\n",
    "\n",
    "print(f'\\nParameters:\\n{lr.intercept_:10.4f} +')\n",
    "for i, parameter in enumerate(selected_model_terms):\n",
    "    print(f'{lr.coef_[i]:10.4f} * {parameter}')\n",
    "\n",
    "print(f\"\\nTraining R2  = {lr.score(x_train, y_train):.3f}\")\n",
    "print(f'Training Q2  = {q2:.3f}')\n",
    "print(f\"Training MAE = {metrics.mean_absolute_error(y_train,y_predictions_train):.3f}\")\n",
    "print(f'Training K-fold R2 = {statistics.mean(k_fold_scores):.3f} (+/- {statistics.stdev(k_fold_scores) ** 2:.3f})')\n",
    "\n",
    "if len(x_test) > 0: # In case there is no test set\n",
    "    print(f\"\\nTest R2      = {mlr_utils.external_r2(y_test,y_predictions_test,y_train):.3f}\")\n",
    "    print(f'Test MAE     = {metrics.mean_absolute_error(y_test,y_predictions_test):.3f}')\n",
    "\n",
    "print(f\"\\nValidation R2      = {mlr_utils.external_r2(y_validate,y_predictions_validate,y_train):.3f}\")\n",
    "print(f'Validation MAE     = {metrics.mean_absolute_error(y_validate,y_predictions_validate):.3f}')\n",
    "\n",
    "# Print an insult if necessary\n",
    "if len(x_test) > 0: # In case there is no test set\n",
    "    train_r2 = lr.score(x_train, y_train)\n",
    "    test_r2 = lr.score(x_test, y_test)\n",
    "    if train_r2 - test_r2 > 0.35 or train_r2 < 0.4 or test_r2 < 0.2 or q2 < 0:\n",
    "        print(\"\\n\" + random.choice(insults))\n",
    "    \n",
    "# Plot the model\n",
    "mlr_utils.plot_MLR_model(y_train, y_predictions_train, y_test, y_predictions_test, loo_train, y_predictions_validate=y_predictions_validate, y_validate=y_validate, output_label=RESPONSE_LABEL, plot_xy=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select hotspot to validate\n",
    "hotspot_index = 0\n",
    "\n",
    "# Set to True if you want to see only the virtual screen results plotted\n",
    "hide_training = False\n",
    "\n",
    "save_evaluation = False # Set to True if you want to save the threshold evaluation to an Excel file in the OutputData folder\n",
    "save_filename = \"threshold_evaluation\" # Name of the Excel file to save the threshold evaluation to if save_evaluation is True\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "# EDIT ABOVE THIS LINE UNLESS YOU WANT TO CHANGE THE PLOTTING DETAILS IN hotspot_utils.plot_hotspot(...)\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "hs = best_hotspots[hotspot_index]\n",
    "\n",
    "# Evaluate each validation point on each threshold\n",
    "threshold_evaluations = hs.expand(vs_combined_df)\n",
    "\n",
    "# Get validation stats and display the hotspot\n",
    "hs.get_external_accuracy(vs_combined_df, RESPONSE_LABEL, verbose=True)\n",
    "hotspot_utils.plot_hotspot(hs, vs_combined_df, vs_parameters_df, coloring='binary', output_label=RESPONSE_LABEL, hide_training=hide_training)\n",
    "\n",
    "# Display the threshold evaluations\n",
    "# True means the point is in on the active side of that threshold\n",
    "display(threshold_evaluations)\n",
    "\n",
    "# Save the threshold evaluations to an Excel file if requested\n",
    "if save_evaluation:\n",
    "    threshold_evaluations.to_excel(\"./OutputData/\" + save_filename + \".xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Virtual Screening"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLR Virtual Screening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the outputs for all datapoints in your library read in in the Import Virtaul Screening Data section\n",
    "# Uses the train/test split defined in the Data Preparation section, so you can reset it there if you want to predict without a split\n",
    "\n",
    "model_index = 21 # Index of the model you want to predict the validation set with\n",
    "\n",
    "save_predictions = False # Set to True if you want to save the predictions to an Excel file in the OutputData folder\n",
    "save_filename = \"mlr_predictions\" # Name of the Excel file to save the predictions to if save_predictions is True\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "# EDIT ABOVE THIS LINE\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Scales the parameters if scaling was used in the train/test split cell\n",
    "if use_scaling:\n",
    "    dummy_column = pd.DataFrame({RESPONSE_LABEL: [0] * len(vs_parameters_df)}, index=vs_parameters_df.index)\n",
    "    vs_modeling_data = pd.concat([dummy_column, vs_parameters_df], axis=1)\n",
    "    final_index = vs_modeling_data.index\n",
    "    final_columns = vs_modeling_data.columns\n",
    "    vs_modeling_data = scaler.transform(vs_modeling_data)\n",
    "    vs_modeling_data_df = pd.DataFrame(vs_modeling_data, index = final_index, columns = final_columns)\n",
    "else:\n",
    "    vs_modeling_data_df = vs_combined_df\n",
    "\n",
    "# Select the model to use\n",
    "selected_model_terms = results.loc[model_index, \"Model\"] # Store a tuple of 'xIDs' for the best model\n",
    "\n",
    "# Break up the train/test data into smaller dataframes for easy reference\n",
    "x_train = modeling_data_df.loc[training_set, selected_model_terms] # Dataframe containing just the parameters used in this model for the ligands used in the training set\n",
    "x_test = modeling_data_df.loc[test_set, selected_model_terms] # Dataframe containing just the parameters used in this model for the ligands used in the test set\n",
    "y_train = modeling_data_df.loc[training_set, RESPONSE_LABEL]\n",
    "y_test = modeling_data_df.loc[test_set, RESPONSE_LABEL]\n",
    "\n",
    "x_validate = vs_modeling_data_df.loc[:, selected_model_terms] # Dataframe containing just the parameters used in this model for all ligands in the library\n",
    "\n",
    "# Set up MLR and predict train/test\n",
    "lr = LinearRegression().fit(x_train, y_train)\n",
    "y_predictions_train = lr.predict(x_train)\n",
    "y_predictions_test =  lr.predict(x_test)\n",
    "y_predictions_validate = lr.predict(x_validate)\n",
    "\n",
    "# Calculate q2 and k-fold for the model\n",
    "q2, loo_train = mlr_utils.calculate_q2(x_train, y_train)\n",
    "k_fold_scores = mlr_utils.repeated_k_fold(x_train, y_train, k=5, n=100)\n",
    "\n",
    "# Print a bunch of stats about the model\n",
    "print(f\"\\nSplit method: {split}\")\n",
    "print(f\"Test ratio: {test_ratio}\")\n",
    "\n",
    "print(f'\\nParameters:\\n{lr.intercept_:10.4f} +')\n",
    "for i, parameter in enumerate(selected_model_terms):\n",
    "    print(f'{lr.coef_[i]:10.4f} * {parameter}')\n",
    "\n",
    "print(f\"\\nTraining R2  = {lr.score(x_train, y_train):.3f}\")\n",
    "print(f'Training Q2  = {q2:.3f}')\n",
    "print(f\"Training MAE = {metrics.mean_absolute_error(y_train,y_predictions_train):.3f}\")\n",
    "print(f'Training K-fold R2 = {statistics.mean(k_fold_scores):.3f} (+/- {statistics.stdev(k_fold_scores) ** 2:.3f})')\n",
    "\n",
    "print(f\"\\nTest R2      = {mlr_utils.external_r2(y_test,y_predictions_test,y_train):.3f}\")\n",
    "print(f'Test MAE     = {metrics.mean_absolute_error(y_test,y_predictions_test):.3f}')\n",
    "\n",
    "# Print an insult if necessary\n",
    "train_r2 = lr.score(x_train, y_train)\n",
    "test_r2 = lr.score(x_test, y_test)\n",
    "if train_r2 - test_r2 > 0.35 or train_r2 < 0.4 or test_r2 < 0.2 or q2 < 0:\n",
    "    print(\"\\n\" + random.choice(insults))\n",
    "    \n",
    "# Plot the model\n",
    "print('\\nVIRTUAL SCREEN POINTS HAVE NOT BEEN EXPERIMENTALLY TESTED. THIS VISUALIZATION IS ONLY TO SHOW DISTRIBUTION OF PREDICTIONS.')\n",
    "mlr_utils.plot_MLR_model(y_train, y_predictions_train, y_test, y_predictions_test, loo_train, y_predictions_validate=y_predictions_validate, output_label=RESPONSE_LABEL)\n",
    "\n",
    "if save_predictions:\n",
    "    validation_predictions_df = pd.DataFrame(y_predictions_validate, index=x_validate.index, columns=['Predicted ' + RESPONSE_LABEL])\n",
    "    validation_predictions_df.to_excel(\"./OutputData/\" + save_filename + \".xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold Virtual Screening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select hotspot to screen against\n",
    "hotspot_index = 0\n",
    "\n",
    "# Set to True if you want to see only the virtual screen results plotted\n",
    "hide_training = True\n",
    "\n",
    "save_predictions = False # Set to True if you want to save the predictions to an Excel file in the OutputData folder\n",
    "save_filename = \"Threshold_VS_Predictions\" # Name of the Excel file to save the predictions to if save_predictions is True\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "# EDIT ABOVE THIS LINE\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "hs = best_hotspots[hotspot_index]\n",
    "\n",
    "# Trim down to the parameter values relevant to this hotspot and add binary evaluations from each ligand\n",
    "# True in an xID column indicates that the ligand is predicted to be active by that threshold\n",
    "virtual_screen_report_df = vs_parameters_df.loc[:, hs.threshold_features]\n",
    "# virtual_screen_report_df.columns = [f'{parameter}' for parameter in virtual_screen_report_df.columns]\n",
    "\n",
    "threshold_evaluations = hs.expand(vs_parameters_df)\n",
    "threshold_evaluations.columns = [f'{parameter} Active' for parameter in threshold_evaluations.columns]\n",
    "virtual_screen_report_df = pd.concat([virtual_screen_report_df, threshold_evaluations], axis=1)\n",
    "\n",
    "# Create a column for the final hotspot evaluation if there is more than a single threshold\n",
    "if len(hs.thresholds) > 1:\n",
    "    for ligand in virtual_screen_report_df.index:\n",
    "        evaluation = all([virtual_screen_report_df.loc[ligand, f'{threshold} Active'] for threshold in hs.threshold_features])\n",
    "        virtual_screen_report_df.loc[ligand, 'Full Hotspot Evaluation'] = evaluation\n",
    "\n",
    "display(virtual_screen_report_df)\n",
    "\n",
    "print('The plot below is meant only to show the distribution of the virtual screen since experimental data has not been supplied')\n",
    "hotspot_utils.plot_hotspot(hs, vs_parameters=vs_parameters_df, hide_training=hide_training, coloring='binary', output_label=RESPONSE_LABEL, gradient_color='Oranges')\n",
    "\n",
    "# Save the predictions to an Excel file if requested\n",
    "if save_predictions:\n",
    "    virtual_screen_report_df.to_excel(\"./OutputData/\" + save_filename + \".xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modeling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "314px",
    "width": "313px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "325.016px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": "400"
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 959.666666,
   "position": {
    "height": "981.263px",
    "left": "1534.33px",
    "right": "20px",
    "top": "5px",
    "width": "631.037px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
